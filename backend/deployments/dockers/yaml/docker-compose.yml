services:
  vllm:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:latest}
    container_name: vllm
    shm_size: '4gb'
    ports:
      - "${VLLM_PORT:-8000}:8000"
    volumes:
      - ${VLLM_MODEL_PATH}:/model
    entrypoint: python3 -m vllm.entrypoints.openai.api_server
    command: >
      --model /model
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --served-model-name ${VLLM_MODEL_NAME}
      --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    env_file:
      - /home/anyadmin/docker/.env-vllm

  anythingllm:
    image: ${ANYTHINGLLM_IMAGE:-mintplexlabs/anythingllm:1.8.5}
    container_name: anythingllm
    cap_add:
      - SYS_ADMIN
    restart: unless-stopped
    environment:
      - SERVER_PORT=${ANYTHINGLLM_PORT:-3001}
      - STORAGE_DIR=${ANYTHINGLLM_STORAGE_DIR:-/app/server/storage}
      - LLM_PROVIDER=${LLM_PROVIDER}
      - GENERIC_OPEN_AI_BASE_PATH=${GENERIC_OPEN_AI_BASE_PATH}
      - GENERIC_OPEN_AI_MODEL_PREF=${GENERIC_OPEN_AI_MODEL_PREF}
      - GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT=${GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT}
      - GENERIC_OPEN_AI_MAX_TOKENS=${GENERIC_OPEN_AI_MAX_TOKENS}
      - GENERIC_OPEN_AI_API_KEY=${GENERIC_OPEN_AI_API_KEY}
      - VECTOR_DB=${VECTOR_DB}
      - AUTH_TOKEN=${AUTH_TOKEN}
      - JWT_SECRET=${JWT_SECRET}
      - DISABLE_TELEMETRY=${DISABLE_TELEMETRY}
    volumes:
      - ${ANYTHINGLLM_DATA_PATH}/storage:/app/server/storage
      - ${ANYTHINGLLM_DATA_PATH}/collector/hotdir/:/app/collector/hotdir
      - ${ANYTHINGLLM_DATA_PATH}/collector/outputs/:/app/collector/outputs
    ports:
      - "${ANYTHINGLLM_PORT:-3001}:${ANYTHINGLLM_PORT:-3001}"
    extra_hosts:
      - host.docker.internal:host-gateway
    env_file:
      - /home/anyadmin/docker/.env-anythingllm