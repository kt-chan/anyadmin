package utils

import (
	"flag"
	"fmt"
	"log"
	"os"
	"regexp"
	"strconv"
	"strings"
)

var (
	// DebugMode controls whether to output detailed logs
	DebugMode = false
)

func debugLog(format string, v ...interface{}) {
	if DebugMode {
		log.Printf("[DEBUG] "+format, v...)
	}
}

// ModelConfig å­˜å‚¨æ¨¡å‹çš„åŸºæœ¬é…ç½®
type ModelConfig struct {
	Name                  string  // æ¨¡å‹åç§°
	ParamsBillion         float64 // æ¨¡å‹å‚æ•°ï¼ˆåäº¿ï¼‰
	HiddenSize            int     // éšè—å±‚ç»´åº¦
	NumHiddenLayers       int     // éšè—å±‚æ•°
	NumAttentionHeads     int     // æ³¨æ„åŠ›å¤´æ•°
	MaxPositionEmbeddings int     // æœ€å¤§ä½ç½®ç¼–ç ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰
	HeadDim               int     // æ¯ä¸ªå¤´çš„ç»´åº¦
	NumKeyValueHeads      int     // KVå¤´æ•°ï¼ˆç”¨äºGQAï¼‰
}

// GPUConfig å­˜å‚¨GPUé…ç½®
type GPUConfig struct {
	MemoryGB    float64 // GPUå†…å­˜ï¼ˆGBï¼‰
	Utilization float64 // å†…å­˜åˆ©ç”¨ç‡
	ReservedGB  float64 // ç³»ç»Ÿé¢„ç•™å†…å­˜ï¼ˆGBï¼‰
}

// VLLMConfig vLLMé…ç½®è¾“å‡º
type VLLMConfig struct {
	MaxModelLen         int     `json:"max_model_len"`          // æœ€å¤§æ¨¡å‹é•¿åº¦
	MaxNumSeqs          int     `json:"max_num_seqs"`           // æœ€å¤§å¹¶å‘åºåˆ—æ•°
	MaxNumBatchedTokens int     `json:"max_num_batched_tokens"` // æœ€å¤§æ‰¹å¤„ç†tokensæ•°
	GPUMemoryUtil       float64 `json:"gpu_memory_util"`        // GPUå†…å­˜åˆ©ç”¨ç‡
	SwapSpaceGB         int     `json:"swap_space_gb"`          // äº¤æ¢ç©ºé—´ï¼ˆGBï¼‰
	EnablePrefixCaching bool    `json:"enable_prefix_caching"`  // æ˜¯å¦å¯ç”¨å‰ç¼€ç¼“å­˜
	KVBlockSize         int     `json:"kv_block_size"`          // KVç¼“å­˜å—å¤§å°
}

// HuggingFaceConfig ä»config.jsonè¯»å–çš„åŸå§‹é…ç½®
type HuggingFaceConfig struct {
	Architectures         []string `json:"architectures,omitempty"`
	HiddenSize            int      `json:"hidden_size,omitempty"`
	NumHiddenLayers       int      `json:"num_hidden_layers,omitempty"`
	NumAttentionHeads     int      `json:"num_attention_heads,omitempty"`
	MaxPositionEmbeddings int      `json:"max_position_embeddings,omitempty"`
	IntermediateSize      int      `json:"intermediate_size,omitempty"`
	VocabSize             int      `json:"vocab_size,omitempty"`
	ModelType             string   `json:"model_type,omitempty"`
	TorchDtype            string   `json:"torch_dtype,omitempty"`
	NumKeyValueHeads      int      `json:"num_key_value_heads,omitempty"` // å¯¹äºGQAæ¨¡å‹
	RopeTheta             float64  `json:"rope_theta,omitempty"`
	SlidingWindow         int      `json:"sliding_window,omitempty"` // æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
}

// CalculateConfigParams è®¡ç®—é…ç½®çš„è¾“å…¥å‚æ•°
type CalculateConfigParams struct {
	ModelNameOrPath string
	GPUMemoryGB     float64
	Mode            string
	GPUUtilization  float64
}

// CalculateVLLMConfig è®¡ç®—vLLMé…ç½® (API friendly version)
func CalculateVLLMConfig(params CalculateConfigParams) (VLLMConfig, ModelConfig, error) {
	// é»˜è®¤åˆ©ç”¨ç‡
	if params.GPUUtilization <= 0 {
		params.GPUUtilization = 0.85
	}

	// å¦‚æœå†…å­˜å°äº8GBä¸”åˆ©ç”¨ç‡ä¸ºé»˜è®¤å€¼0.85ï¼Œåˆ™ä¿æŒä¸€è‡´
	if params.GPUMemoryGB < 8 && params.GPUUtilization == 0.85 {
		params.GPUUtilization = 0.85
	}

	// å°è¯•åŠ è½½æ¨¡å‹é…ç½®
	modelConfig := EstimateModelConfigFromName(params.ModelNameOrPath)

	// ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®
	if modelConfig.Name == "" {
		modelConfig.Name = params.ModelNameOrPath
	}

	// åˆ›å»ºGPUé…ç½®
	gpuConfig := GPUConfig{
		MemoryGB:    params.GPUMemoryGB,
		Utilization: params.GPUUtilization,
		ReservedGB:  1.5, // é»˜è®¤é¢„ç•™1.5GBç»™ç³»ç»Ÿ
	}

	// æ ¹æ®æ¨¡å¼è®¡ç®—ä¼˜åŒ–é…ç½®
	var vllmConfig VLLMConfig
	switch strings.ToLower(params.Mode) {
	case "max_token":
		vllmConfig = CalculateMaxTokenConfig(modelConfig, gpuConfig)
	case "max_concurrency":
		vllmConfig = CalculateMaxConcurrencyConfig(modelConfig, gpuConfig)
	case "balanced":
		vllmConfig = CalculateBalancedConfig(modelConfig, gpuConfig)
	default:
		vllmConfig = CalculateBalancedConfig(modelConfig, gpuConfig)
	}

	// æ€»æ˜¯å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆå¯¹æ€§èƒ½æœ‰ç›Šï¼‰
	vllmConfig.EnablePrefixCaching = true

	return vllmConfig, modelConfig, nil
}

func main() {
	// è§£æå‘½ä»¤è¡Œå‚æ•°
	model := flag.String("model", "", "æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„")
	gpuMemoryStr := flag.String("gpu_memory", "8G", "GPUå†…å­˜")
	mode := flag.String("mode", "balanced", "ä¼˜åŒ–æ¨¡å¼")
	utilization := flag.Float64("utilization", 0.85, "GPUå†…å­˜åˆ©ç”¨ç‡")
	enableSwap := flag.Bool("enable_swap", false, "æ˜¯å¦å¯ç”¨äº¤æ¢ç©ºé—´")

	flag.Parse()

	if *model == "" {
		os.Exit(1)
	}

	gpuMemoryGB, _ := parseMemoryString(*gpuMemoryStr)

	params := CalculateConfigParams{
		ModelNameOrPath: *model,
		GPUMemoryGB:     gpuMemoryGB,
		Mode:            *mode,
		GPUUtilization:  *utilization,
	}

	vllmConfig, modelConfig, err := CalculateVLLMConfig(params)
	if err != nil {
		log.Fatalf("è®¡ç®—é…ç½®å¤±è´¥: %v", err)
	}

	if *enableSwap && gpuMemoryGB < 16 {
		vllmConfig.SwapSpaceGB = 8
	}

	gpuConfig := GPUConfig{
		MemoryGB:    gpuMemoryGB,
		Utilization: *utilization,
		ReservedGB:  1.5,
	}

	printConfig(modelConfig, gpuConfig, vllmConfig, *mode)
	printVLLMCommand(*model, vllmConfig)
}

// parseMemoryString è§£æå†…å­˜å­—ç¬¦ä¸²
func parseMemoryString(memoryStr string) (float64, error) {
	re := regexp.MustCompile(`^(\d+(?:\.\d+)?)\s*([GT]?B?)$`)
	matches := re.FindStringSubmatch(strings.ToUpper(memoryStr))

	if matches == nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„å†…å­˜æ ¼å¼")
	}

	value, _ := strconv.ParseFloat(matches[1], 64)
	unit := matches[2]
	if strings.Contains(unit, "T") {
		value *= 1024
	}

	return value, nil
}

func estimateModelParams(hfConfig HuggingFaceConfig) float64 {
	if hfConfig.HiddenSize == 0 || hfConfig.NumHiddenLayers == 0 {
		return extractParamsFromName(hfConfig.ModelType)
	}
	vocabSize := hfConfig.VocabSize
	if vocabSize == 0 {
		vocabSize = 100000
	}
	hiddenSize := float64(hfConfig.HiddenSize)
	numLayers := float64(hfConfig.NumHiddenLayers)
	embeddingParams := float64(vocabSize) * hiddenSize
	transformerParamsPerLayer := 12.0 * hiddenSize * hiddenSize
	transformerParams := numLayers * transformerParamsPerLayer
	return (embeddingParams + transformerParams) / 1e9
}

func extractParamsFromName(modelName string) float64 {
	modelNameLower := strings.ToLower(modelName)
	patterns := []*regexp.Regexp{
		regexp.MustCompile(`(\d+(?:\.\d+)?)[Bb]`),
		regexp.MustCompile(`-(\d+)b`),
		regexp.MustCompile(`(\d+)[Bb]`),
		regexp.MustCompile(`(\d+(?:\.\d+)?)b`),
		regexp.MustCompile(`qwen3-(\d+(?:\.\d+)?)b`),
	}
	for _, pattern := range patterns {
		matches := pattern.FindStringSubmatch(modelNameLower)
		if matches != nil {
			params, err := strconv.ParseFloat(matches[1], 64)
			if err == nil {
				return params
			}
		}
	}
	return 7.0
}

func EstimateModelConfigFromName(modelName string) ModelConfig {
	paramsBillion := extractParamsFromName(modelName)
	var hiddenSize, numLayers, numHeads, maxPosEmbeddings int
	switch {
	case paramsBillion <= 1.0:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 1024, 12, 12, 8192
	case paramsBillion <= 1.7:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 1536, 16, 12, 131072
	case paramsBillion <= 3.0:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 2048, 24, 16, 32768
	case paramsBillion <= 7.0:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 4096, 32, 32, 32768
	case paramsBillion <= 13.0:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 5120, 40, 40, 32768
	case paramsBillion <= 34.0:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 8192, 60, 64, 131072
	default:
		hiddenSize, numLayers, numHeads, maxPosEmbeddings = 8192, 80, 64, 131072
	}
	return ModelConfig{
		Name:                  modelName,
		ParamsBillion:         paramsBillion,
		HiddenSize:            hiddenSize,
		NumHiddenLayers:       numLayers,
		NumAttentionHeads:     numHeads,
		MaxPositionEmbeddings: maxPosEmbeddings,
		HeadDim:               hiddenSize / numHeads,
		NumKeyValueHeads:      numHeads,
	}
}

func calculateModelWeightMemory(model ModelConfig) float64 {
	return model.ParamsBillion * 2.0
}

func calculateKVCachePerToken(model ModelConfig) float64 {
	kvChannels := model.NumKeyValueHeads
	if kvChannels == 0 {
		kvChannels = model.NumAttentionHeads
	}
	safetyFactor := 1.20
	return 2.0 * float64(model.NumHiddenLayers) * float64(kvChannels) * float64(model.HeadDim) * 2.0 * safetyFactor
}

// CalculateMaxTokenConfig è®¡ç®—æœ€å¤§åŒ–åºåˆ—é•¿åº¦çš„é…ç½®
func CalculateMaxTokenConfig(model ModelConfig, gpu GPUConfig) VLLMConfig {
	availableMemoryGB := gpu.MemoryGB * gpu.Utilization
	weightMemoryGB := calculateModelWeightMemory(model)
	systemReservedGB := 1.5 // Standard reservation

	// Fix 1: Add a safety buffer (0.85) to account for fragmentation/activation overhead
	kvCacheMemoryGB := (availableMemoryGB - weightMemoryGB - systemReservedGB) * 0.85

	if kvCacheMemoryGB < 0.5 {
		kvCacheMemoryGB = 0.5
	}

	kvCachePerTokenBytes := calculateKVCachePerToken(model)
	kvCachePerTokenGB := kvCachePerTokenBytes / (1024 * 1024 * 1024)

	// Total physical tokens we can fit in RAM
	totalTokenCapacity := int(kvCacheMemoryGB / kvCachePerTokenGB)

	// Calculate MaxModelLen
	maxTokens := totalTokenCapacity
	if model.MaxPositionEmbeddings > 0 && maxTokens > model.MaxPositionEmbeddings {
		maxTokens = model.MaxPositionEmbeddings
	}

	// Hard floor
	if maxTokens < 2048 {
		maxTokens = 2048
	}

	// Align to 128 for hardware efficiency
	maxTokens = (maxTokens / 128) * 128

	// Fix 2: Strict check against capacity again after alignment
	if maxTokens > totalTokenCapacity {
		maxTokens = (totalTokenCapacity / 128) * 128
	}

	maxConcurrency := 1
	if kvCacheMemoryGB > 4.0 {
		maxConcurrency = 2
	}

	// Fix 3: MaxNumBatchedTokens logic
	// It must be at least equal to MaxModelLen to process a full context prompt
	batchTokens := maxTokens

	// Ensure batch tokens doesn't exceed physical limit
	if batchTokens > totalTokenCapacity {
		batchTokens = totalTokenCapacity
	}

	return VLLMConfig{
		MaxModelLen:         maxTokens,
		MaxNumSeqs:          maxConcurrency,
		MaxNumBatchedTokens: batchTokens,
		GPUMemoryUtil:       gpu.Utilization,
		KVBlockSize:         16,
	}
}

// CalculateMaxConcurrencyConfig è®¡ç®—æœ€å¤§åŒ–å¹¶å‘æ•°çš„é…ç½®
func CalculateMaxConcurrencyConfig(model ModelConfig, gpu GPUConfig) VLLMConfig {
	availableMemoryGB := gpu.MemoryGB * gpu.Utilization
	weightMemoryGB := calculateModelWeightMemory(model)
	systemReservedGB := 1.5

	// Safety buffer
	kvCacheMemoryGB := (availableMemoryGB - weightMemoryGB - systemReservedGB) * 0.85
	if kvCacheMemoryGB < 1.0 {
		kvCacheMemoryGB = 1.0
	}

	kvCachePerTokenBytes := calculateKVCachePerToken(model)
	kvCachePerTokenGB := kvCachePerTokenBytes / (1024 * 1024 * 1024)
	totalTokenCapacity := int(kvCacheMemoryGB / kvCachePerTokenGB)

	seqLen := 4096
	if gpu.MemoryGB < 8 {
		seqLen = 2048
	} else if gpu.MemoryGB > 16 {
		seqLen = 8192
	}

	// Clamp seqLen to model limits
	if model.MaxPositionEmbeddings > 0 && seqLen > model.MaxPositionEmbeddings {
		seqLen = model.MaxPositionEmbeddings
	}

	// Fix: Clamp seqLen to physical capacity
	if seqLen > totalTokenCapacity {
		seqLen = totalTokenCapacity
	}

	// Alignment
	seqLen = (seqLen / 128) * 128

	maxConcurrency := totalTokenCapacity / seqLen
	if maxConcurrency < 1 {
		maxConcurrency = 1
	} else if maxConcurrency > 256 {
		maxConcurrency = 256
	}

	// Fix: Batched tokens calculation
	batchTokens := seqLen * maxConcurrency

	// Strict clamp: Batch tokens cannot exceed total physical tokens
	if batchTokens > totalTokenCapacity {
		batchTokens = totalTokenCapacity
	}

	// vLLM recommendation: batch tokens should be at least max_model_len
	if batchTokens < seqLen {
		batchTokens = seqLen
	}

	return VLLMConfig{
		MaxModelLen:         seqLen,
		MaxNumSeqs:          maxConcurrency,
		MaxNumBatchedTokens: batchTokens,
		GPUMemoryUtil:       gpu.Utilization,
		KVBlockSize:         16,
	}
}

// CalculateBalancedConfig è®¡ç®—å¹³è¡¡é…ç½®
func CalculateBalancedConfig(model ModelConfig, gpu GPUConfig) VLLMConfig {
	availableMemoryGB := gpu.MemoryGB * gpu.Utilization
	weightMemoryGB := calculateModelWeightMemory(model)
	systemReservedGB := 1.5

	// Fix 1: Safety buffer of 0.85 (15% headroom for activation overhead/fragmentation)
	kvCacheMemoryGB := (availableMemoryGB - weightMemoryGB - systemReservedGB) * 0.85

	if kvCacheMemoryGB < 1.0 {
		kvCacheMemoryGB = 1.0
	}

	kvCachePerTokenBytes := calculateKVCachePerToken(model)
	kvCachePerTokenGB := kvCachePerTokenBytes / (1024 * 1024 * 1024)

	// This is the hard physical limit of tokens
	totalTokenCapacity := int(kvCacheMemoryGB / kvCachePerTokenGB)

	var seqLen int
	if gpu.MemoryGB < 6 {
		seqLen = 2048
	} else if gpu.MemoryGB < 12 {
		seqLen = 4096
	} else if gpu.MemoryGB < 24 {
		seqLen = 8192
	} else {
		seqLen = 16384
	}

	if model.MaxPositionEmbeddings > 0 && seqLen > model.MaxPositionEmbeddings {
		seqLen = model.MaxPositionEmbeddings
	}

	// Fix 2: STRICT Clamping. If capacity is 17680, seqLen cannot be 32768
	if seqLen > totalTokenCapacity {
		seqLen = totalTokenCapacity
	}

	// Align to 256
	seqLen = (seqLen / 256) * 256

	// Determine concurrency
	maxConcurrency := totalTokenCapacity / seqLen
	if maxConcurrency > 8 {
		maxConcurrency = 8
	} else if maxConcurrency < 2 {
		maxConcurrency = 2
	}

	// Fix 3: Calculate batch tokens
	batchTokens := seqLen * 2

	// CRITICAL FIX for your specific error:
	// "max-num-batched-tokens is still far too large"
	// We must ensure batchTokens never exceeds totalTokenCapacity
	if batchTokens > totalTokenCapacity {
		batchTokens = totalTokenCapacity
	}

	// Ensure we can at least process one full sequence
	if batchTokens < seqLen {
		batchTokens = seqLen
	}

	// Cap at 32k or 16k if needed by model, but hardware limit (totalTokenCapacity) takes precedence
	if batchTokens > 32768 {
		batchTokens = 32768
	}

	return VLLMConfig{
		MaxModelLen:         seqLen,
		MaxNumSeqs:          maxConcurrency,
		MaxNumBatchedTokens: batchTokens,
		GPUMemoryUtil:       gpu.Utilization,
		KVBlockSize:         16,
	}
}

func printConfig(model ModelConfig, gpu GPUConfig, vllm VLLMConfig, mode string) {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘                 vLLM é…ç½®ä¼˜åŒ–å·¥å…·                        â•‘")
	fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
	fmt.Printf("â•‘ æ¨¡å‹: %-50s â•‘\n", model.Name)
	fmt.Printf("â•‘ GPUå†…å­˜: %.1fGB | æ¨¡å¼: %-15s | åˆ©ç”¨ç‡: %.2f â•‘\n",
		gpu.MemoryGB, mode, gpu.Utilization)
	fmt.Println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
	fmt.Printf("â•‘ --max-model-len          %-10d (æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦)       â•‘\n", vllm.MaxModelLen)
	fmt.Printf("â•‘ --max-num-seqs           %-10d (æœ€å¤§å¹¶å‘è¯·æ±‚æ•°)       â•‘\n", vllm.MaxNumSeqs)
	fmt.Printf("â•‘ --max-num-batched-tokens %-10d (æ‰¹å¤„ç†tokensæ•°)      â•‘\n", vllm.MaxNumBatchedTokens)
	fmt.Printf("â•‘ --gpu-memory-utilization %-10.2f (GPUå†…å­˜åˆ©ç”¨ç‡)      â•‘\n", vllm.GPUMemoryUtil)
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
}

func printVLLMCommand(model string, config VLLMConfig) {
	fmt.Println("\nğŸš€ ç”Ÿæˆçš„ vLLM å¯åŠ¨å‘½ä»¤:")
	fmt.Printf("vllm serve %s \\\n", model)
	fmt.Printf("    --max-model-len %d \\\n", config.MaxModelLen)
	fmt.Printf("    --max-num-seqs %d \\\n", config.MaxNumSeqs)
	fmt.Printf("    --max-num-batched-tokens %d \\\n", config.MaxNumBatchedTokens)
	fmt.Printf("    --gpu-memory-utilization %.2f\n", config.GPUMemoryUtil)
}
